---
title: "Web Scraping and Data Extraction – ScrapeThisSite"
author: "Ignacio Vélez Ocampo"
date: today
format: html
jupyter: python3
execute:
  dir: project
  echo: true
  warning: false
  message: false
---

## 1. Project Objective

This notebook represents the **first stage** of the *Data Integration Pipeline – From Web to Database*. It demonstrates a **reproducible web data extraction workflow** using [ScrapeThisSite.com](https://www.scrapethissite.com), an educational platform designed for ethical scraping practice.

The goal is to extract structured information about **countries** — including *name, capital, population,* and *area* — and prepare it as a clean dataset for subsequent integration with API and SQL-based data sources.

------------------------------------------------------------------------

## 2. Ethical and Responsible Scraping

Before extracting any data, we validate that our workflow complies with **responsible scraping principles**:

-   The target section `/pages/simple/` is allowed under the site’s [`robots.txt`](https://www.scrapethissite.com/robots.txt).
-   A clear **User-Agent header** is included to identify the request.
-   A **polite delay (1s)** is implemented to avoid server overload.
-   Only **public and non-sensitive data** are collected.

These steps ensure that the scraping process is **transparent, ethical, and compliant** with web data usage guidelines.

------------------------------------------------------------------------

## 3. Environment Setup

We import the required Python libraries for HTML requests, parsing, and structured data handling:

-   **requests** → retrieve HTML from the target page
-   **BeautifulSoup (bs4)** → parse and extract data elements
-   **pandas** → structure, clean, and export tabular data
-   **time** → manage polite scraping delays

```{python}
# 3. Environment Setup ---------------------------------------------------------
"""
Initialize core libraries and environment settings required for web scraping
and data handling.
"""

import requests                      # Retrieve HTML content from the target URL
from bs4 import BeautifulSoup        # Parse and navigate structured HTML
import pandas as pd                  # Create, clean, and export tabular data
import time                          # Apply polite delays during scraping
from pathlib import Path             # Manage file system paths cross-platform

# Configure pandas display options for better readability in outputs
pd.set_option("display.max_columns", None)
pd.set_option("display.width", 120)

```

------------------------------------------------------------------------

## 4. Fetch and Parse HTML

We request the public `/pages/simple/` endpoint from *ScrapeThisSite*, identify ourselves with a custom User-Agent, and parse the response using BeautifulSoup.

```{python}
# 4. Fetch and Parse HTML ------------------------------------------------------
"""
Fetch the HTML content from ScrapeThisSite, applying polite scraping practices,
validating response integrity, and saving a local copy for reproducibility.
"""

import requests, time
from bs4 import BeautifulSoup
from pathlib import Path

# Define target URL and polite headers
url = "https://www.scrapethissite.com/pages/simple/"
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

# Determine project root dynamically (compatible with Quarto/Jupyter execution)
project_root = Path.cwd().resolve()
for _ in range(3):
    if (project_root / "data").exists():
        break
    project_root = project_root.parent

# Define paths for raw data and cached HTML
output_dir = project_root / "data" / "raw"
output_dir.mkdir(parents=True, exist_ok=True)
html_path = output_dir / "scrapethissite_page.html"

# Load cached HTML if available; otherwise fetch it from the web
if html_path.exists():
    with open(html_path, "r", encoding="utf-8") as f:
        html = f.read()
else:
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        html = response.text

        # Basic length validation to detect incomplete responses
        if len(html) < 5000:
            raise ValueError("HTML content too short — possible incomplete fetch.")

        # Save HTML locally for reproducibility
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(html)

    except (requests.exceptions.RequestException, ValueError) as e:
        print(f"Error fetching or validating HTML: {e}")
        html = None

# Parse HTML content if successfully retrieved
if html:
    soup = BeautifulSoup(html, "html.parser")

    # Flexible verification: ensure expected structure exists, without breaking execution
    if not soup.select("div.country"):
        print("Warning: No <div class='country'> blocks found in HTML. Proceeding anyway.")
    else:
        print(f"HTML loaded successfully — Page title: {soup.title.string if soup.title else 'No title found'}")
else:
    raise RuntimeError("HTML content could not be loaded or validated.")

```

------------------------------------------------------------------------

## 5. Extract and Structure Country Information

Each country’s details are contained in `<div class='country'>` blocks, with sub-elements providing specific fields:

-   `.country-name`
-   `.country-capital`
-   `.country-population`
-   `.country-area`

We loop through these and collect all values into a structured list.

```{python}
# 5. Extract and Structure Country Information ---------------------------------
"""
Extract country-level information from the parsed HTML structure, including
name, capital, population, and area, and store it in a list of records.
"""

# Initialize container and apply a short polite delay before scraping
countries = []
time.sleep(1)

# Loop through each country block and extract relevant fields
for div in soup.select("div.country"):
    try:
        name = div.select_one(".country-name").get_text(strip=True).title()
        capital = div.select_one(".country-capital").get_text(strip=True).title()
        population = div.select_one(".country-population").get_text(strip=True)
        area = div.select_one(".country-area").get_text(strip=True)
        countries.append([name, capital, population, area])
    except AttributeError:
        continue  # Skip malformed entries

if not countries:
    raise ValueError("No country records were extracted from the HTML content.")

```

------------------------------------------------------------------------

## 6. Create and Clean the DataFrame

We transform the raw scraped data into a pandas DataFrame, clean numerical fields, and ensure consistent formatting before export.

```{python}
# 6. Build and Clean DataFrame -------------------------------------------------
"""
Convert the extracted country records into a structured pandas DataFrame,
enforce consistent data types, and validate numerical fields for later analysis.
"""

# Create DataFrame from extracted records
scraped_df = pd.DataFrame(
    countries,
    columns=["name", "capital", "population", "area"]
)

# Standardize column names for SQL compatibility
scraped_df.columns = ["country_name", "capital", "population", "area_km2"]

# Clean and normalize numeric columns while preserving missing values
scraped_df["population"] = (
    scraped_df["population"]
    .str.replace(",", "", regex=False)
    .replace("", None)
)
scraped_df["area_km2"] = (
    scraped_df["area_km2"]
    .str.replace(",", "", regex=False)
    .replace("", None)
)

# Convert columns to consistent and explicit data types
scraped_df = scraped_df.astype({
    "country_name": "string",
    "capital": "string",
    "population": "Int64",   # allows missing values (nullable integer)
    "area_km2": "float"
})

# Display dataset structure and sample rows for validation
print(f"DataFrame created successfully with {len(scraped_df)} rows and {len(scraped_df.columns)} columns.")
print(scraped_df.dtypes)
scraped_df.head()

```

------------------------------------------------------------------------

## 7. Export Dataset and Metadata

The cleaned dataset and metadata (source URL, timestamp) are saved under the project’s `data/raw/` folder for traceability and offline reproducibility.

```{python}
# 7. Export Dataset and Metadata -----------------------------------------------
"""
Export the cleaned dataset to CSV and generate a metadata JSON file including
source, timestamp, record count, and column information for reproducibility.
"""

from datetime import datetime
import json
from pathlib import Path

# Ensure the output directory exists
output_dir.mkdir(parents=True, exist_ok=True)

# Define export path
csv_path = output_dir / "scrapethissite_countries.csv"

# Export the cleaned dataset to CSV
scraped_df.to_csv(csv_path, index=False, encoding="utf-8")

# Build metadata for reproducibility and traceability
metadata = {
    "source": url,
    "timestamp": datetime.now().isoformat(),
    "records": len(scraped_df),
    "columns": list(scraped_df.columns),
    "output_file": str(csv_path.name)
}

# Save metadata as JSON
metadata_path = output_dir / "metadata_scrape.json"
with open(metadata_path, "w", encoding="utf-8") as f:
    json.dump(metadata, f, indent=2)

# Validate export integrity (non-blocking warnings)
if not csv_path.exists() or csv_path.stat().st_size == 0:
    print(f"Warning: CSV export may have failed or file is empty: {csv_path}")

if scraped_df.empty:
    print("Warning: Exported dataset appears empty. Check scraping process.")

# Report process summary
print("Scraping and export process completed successfully.")
print(f"Data saved to: {csv_path}")
print(f"Metadata saved to: {metadata_path}")
print(f"Records exported: {metadata['records']}")
print(f"Working directory: {project_root}")
print(f"Export completed: {metadata['records']} rows × {len(metadata['columns'])} columns.")

```

------------------------------------------------------------------------

## 8. Summary

This notebook constitutes **Stage 1 of the Data Integration Pipeline – From Web to Database**. It creates a reproducible, structured dataset of countries that serves as the foundation for the next stages:

-   **Stage 2:** Weather data collection via OpenWeather API
-   **Stage 3:** SQL integration with World Bank GDP indicators

The resulting files — `scrapethissite_countries.csv`, `metadata_scrape.json`, and `scrapethissite_page.html` — provide the validated inputs required for the following steps in the pipeline.

------------------------------------------------------------------------