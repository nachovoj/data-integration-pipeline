---
title: "Weather Data Collection – OpenWeather API"
author: "Ignacio Vélez Ocampo"
date: today
format: html
jupyter: python3
execute:
  dir: project
  echo: true
  warning: false
  message: false
---

## 1. Project Objective

This notebook represents **Stage 2 of the Data Integration Pipeline – From Web to Database**. It implements a **reproducible API-based data collection workflow** using the [OpenWeather API](https://openweathermap.org/api), enriching the country dataset generated in Stage 1 with live **temperature** and **humidity** metrics for global capitals.

The resulting dataset is validated, harmonized, and stored together with the scraped data in a local **SQLite database**, preparing the foundation for **Stage 3 (SQL integration and analysis)**.

------------------------------------------------------------------------

## 2. Environment Setup

We begin by importing all required libraries for API requests, data manipulation, and file management. These ensure a consistent and reproducible execution environment across systems.

-   **requests** – send HTTP requests to the OpenWeather API
-   **pandas** – manage tabular structures and clean results
-   **pycountry** – retrieve standardized ISO2 country codes
-   **dotenv / os / pathlib** – manage environment variables and project paths
-   **sqlite3** – store results in a local relational database

```{python}
# 2. Environment Setup ---------------------------------------------------------
"""
Initialize core libraries and environment settings required for API access,
data transformation, and reproducible storage.
"""

import os
import time
import sqlite3
import requests
import pandas as pd
import pycountry
from datetime import datetime
from typing import Optional
from pathlib import Path
from dotenv import load_dotenv, dotenv_values

# Configure pandas display options
pd.set_option("display.max_columns", None)
pd.set_option("display.width", 120)

import warnings
warnings.filterwarnings("ignore", category=UserWarning)
```

------------------------------------------------------------------------

## 3. Load API Key Securely

To protect credentials, the OpenWeather API key is stored in a local `.env` file and not committed to the public repository.

This approach ensures both **security** and **portability** across different execution environments.

```{python}
# 3. Load API Key Securely -----------------------------------------------------
"""
Load the OpenWeather API key from a .env file, ensuring compatibility across
macOS, Windows, and Jupyter/Quarto execution environments.
"""

def find_project_root(marker: str = "data") -> Path:
    """Locate project root by traversing upward until the given marker folder is found."""
    path = Path.cwd().resolve()
    for _ in range(6):
        if (path / marker).exists():
            return path
        path = path.parent
    return Path.cwd().resolve()


# Detect project root and .env path
project_root = find_project_root()
env_path = project_root / ".env"

# Load environment variables
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=False)
else:
    load_dotenv()

API_KEY = os.getenv("OPENWEATHER_API_KEY")

# Fallback for macOS kernel isolation
if not API_KEY and env_path.exists():
    API_KEY = dotenv_values(env_path).get("OPENWEATHER_API_KEY")

# Validate presence of API key
if not API_KEY:
    raise ValueError(f"API key not found. Define OPENWEATHER_API_KEY in {env_path}.")

print(f"API key loaded successfully from: {env_path if env_path.exists() else 'system environment'}")

```

------------------------------------------------------------------------

## 4. Prepare Dataset with ISO2 Country Codes

The dataset `scrapethissite_countries.csv`, generated in the previous notebook, contains country names and capitals but no ISO codes.

We enrich this dataset by appending the correct ISO2 country code to each country using the **pycountry** library.

```{python}
# 4. Prepare Dataset with ISO2 Country Codes -----------------------------------
"""
Load the scraped dataset of countries and capitals, match each country to its
ISO-2 code using pycountry, and prepare a clean list of capital–country pairs
for API requests.
"""

# Locate the scraped dataset
data_path = Path("data/raw/scrapethissite_countries.csv")
if not data_path.exists():
    data_path = Path("../data/raw/scrapethissite_countries.csv")

scraped_df = pd.read_csv(data_path)
print(f"Loaded {len(scraped_df)} records from scraped dataset.")

# Match each country name to its ISO2 code
import unicodedata

def get_country_code(country_name: str) -> str | None:
    """Return ISO2 code for given country name (robust to accents)."""
    try:
        normalized = unicodedata.normalize("NFKD", country_name).encode("ascii", "ignore").decode("utf-8")
        return pycountry.countries.lookup(normalized).alpha_2
    except LookupError:
        return None

scraped_df["country_code"] = scraped_df["country_name"].apply(get_country_code)

# Manual corrections for ambiguous names
manual_fixes = {
    "Congo (Brazzaville)": "CG",
    "Congo (Kinshasa)": "CD",
    "Côte d'Ivoire": "CI",
    "Bolivia": "BO",
    "Syria": "SY"
}

scraped_df["country_code"] = scraped_df.apply(
    lambda row: manual_fixes.get(row["country_name"], row["country_code"]),
    axis=1
)

# Clean empty capitals
scraped_df = scraped_df.dropna(subset=["capital"])
scraped_df = scraped_df[scraped_df["capital"].str.strip() != ""]

capitals_clean = scraped_df[["capital", "country_code"]].copy()
print(f"Prepared {len(capitals_clean)} capital–country pairs for API querying.")
capitals_clean.head()

```

### 4.1 Handle Missing Country Codes

Some records remain unmatched due to differences in naming conventions or territorial designations. We fix known cases manually (e.g., Vatican City, São Tomé) and remove any entries still missing valid codes.

```{python}
# 4.1 Handle Missing Country Codes --------------------------------------------
"""
Fix missing ISO2 country codes for known exceptions (territories or special
spellings) and drop any remaining invalid records.
"""

# Identify records without a valid ISO2 code
missing = capitals_clean[capitals_clean["country_code"].isna()]
print(f"Detected {len(missing)} capitals without ISO2 code.")

# Manual mapping for exceptional or unrecognized cases
manual_codes = {
    "Bandar Seri Begawan": "BN",
    "Kinshasa": "CD",
    "Yamoussoukro": "CI",
    "Praia": "CV",
    "Willemstad": "CW",
    "Palikir": "FM",
    "Majuro": "MH",
    "Skopje": "MK",
    "Naypyitaw": "MM",
    "Moscow": "RU",
    "São Tomé": "ST",
    "Pristina": "XK",
    "Vatican City": "VA",
    "Ankara": "TR"
}

# Apply manual fixes
capitals_clean["country_code"] = capitals_clean.apply(
    lambda row: manual_codes.get(row["capital"], row["country_code"]),
    axis=1
)

# Drop any remaining missing values
capitals_clean = capitals_clean.dropna(subset=["country_code"])

print(f"Cleaned dataset includes {len(capitals_clean)} valid capital–country pairs.")
capitals_clean.head()

```

------------------------------------------------------------------------

## 5. Fetch Weather Data from OpenWeather API

We now connect to the **OpenWeather API** to collect live weather data for each capital–country pair. Each query retrieves two metrics: current **temperature (°C)** and **humidity (%)**.

A 1-second delay is applied between requests to ensure compliance with API rate limits and avoid server overload.

```{python}
# 5. Fetch Weather Data from OpenWeather API ----------------------------------
"""
Retrieve live temperature (°C) and humidity (%) for each capital–country pair
using the OpenWeather API. Implements retry logic with exponential backoff
and polite delays to comply with API rate limits.
"""

import requests
import time
from datetime import datetime

BASE_URL = "https://api.openweathermap.org/data/2.5/weather"

def fetch_city_weather(
    session: requests.Session,
    city: str,
    country_code: str,
    api_key: str,
    timeout: int = 10,
    max_retries: int = 3
) -> Optional[dict]:
    """Request weather data for a given capital with retry logic on rate limits."""
    backoff = 2
    for attempt in range(1, max_retries + 1):
        try:
            # Build and send request
            resp = session.get(
                BASE_URL,
                params={"q": f"{city},{country_code}", "appid": api_key, "units": "metric"},
                timeout=timeout,
            )
            status = resp.status_code

            # Parse successful response
            if status == 200:
                data = resp.json()
                main = data.get("main", {})
                coord = data.get("coord", {})
                return {
                    "capital": city,
                    "country_code": country_code,
                    "temp_celsius": main.get("temp"),
                    "humidity": main.get("humidity"),
                    "api_city": data.get("name"),
                    "lat": coord.get("lat"),
                    "lon": coord.get("lon"),
                    "timestamp": datetime.now().isoformat(timespec="seconds"),
                }

            # Retry on rate limit or server errors
            if status == 429 or 500 <= status < 600:
                time.sleep(backoff)
                backoff *= 2
                continue

            # Skip non-retryable responses
            print(f"Skipping {city} ({country_code}) — HTTP {status}")
            return None

        except requests.RequestException:
            # Retry on network-level errors
            time.sleep(backoff)
            backoff *= 2

    print(f"Failed after {max_retries} attempts: {city} ({country_code})")
    return None


# Prepare list of valid capital–country pairs
pairs = (
    capitals_clean[["capital", "country_code"]]
    .dropna()
    .drop_duplicates()
    .itertuples(index=False, name=None)
)

records = []

# Iterate through all pairs with polite delay
with requests.Session() as s:
    for i, (city, cc) in enumerate(pairs, start=1):
        record = fetch_city_weather(s, city=city, country_code=cc, api_key=API_KEY)
        if record:
            records.append(record)
        time.sleep(1)
        if i % 25 == 0:
            print(f"Progress: {i} requests processed, {len(records)} successful")

print(f"Weather data collected successfully for {len(records)} capitals.")

```

------------------------------------------------------------------------

## 6. Export Clean Weather Dataset

Once all API responses are collected, we transform the data into a structured pandas DataFrame.

The resulting table is validated and stored as `weather_data.csv` in the `data/processed/` folder for future use. This ensures that the weather dataset can be reused or merged without re-running the API requests.

```{python}
# 6. Export Clean Weather Dataset ---------------------------------------------
"""
Convert the collected API responses into a structured pandas DataFrame,
validate schema and types, and export both the dataset and metadata for
reproducibility. If no records are found, issue a warning and skip export
to preserve pipeline continuity.
"""

import json
from datetime import datetime
from pathlib import Path

# Define project root dynamically (compatible with Quarto/Jupyter execution)
project_root = Path.cwd().resolve()
for _ in range(3):
    if (project_root / "data").exists():
        break
    project_root = project_root.parent

output_dir = project_root / "data" / "processed"
output_dir.mkdir(parents=True, exist_ok=True)

csv_path = output_dir / "weather_data.csv"
metadata_path = output_dir / "metadata_weather.json"

# Create DataFrame from collected records
weather_df = pd.DataFrame(records)

# Handle empty dataset gracefully
if weather_df.empty:
    print("Warning: No weather records were collected. Skipping export step.")
else:
    # Enforce expected schema
    expected_cols = {
        "capital", "country_code", "temp_celsius", "humidity",
        "api_city", "lat", "lon", "timestamp"
    }
    missing_cols = expected_cols - set(weather_df.columns)
    if missing_cols:
        print(f"Warning: Missing expected columns: {missing_cols}")

    # Normalize numeric columns for consistency
    weather_df["temp_celsius"] = pd.to_numeric(weather_df["temp_celsius"], errors="coerce")
    weather_df["humidity"] = pd.to_numeric(weather_df["humidity"], errors="coerce")
    weather_df["lat"] = pd.to_numeric(weather_df["lat"], errors="coerce")
    weather_df["lon"] = pd.to_numeric(weather_df["lon"], errors="coerce")

    # Export dataset
    weather_df.to_csv(csv_path, index=False, encoding="utf-8")

    # Generate metadata for traceability
    metadata = {
        "source": "OpenWeather API",
        "timestamp": datetime.now().isoformat(),
        "records": len(weather_df),
        "columns": list(weather_df.columns),
        "output_file": csv_path.name
    }

    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2)

    # Post-export validation (non-blocking)
    if not csv_path.exists() or csv_path.stat().st_size == 0:
        print(f"Warning: Export failed or file is empty: {csv_path}")

    # Print summary
    print("\n--- Export Summary ---")
    print("Weather dataset exported successfully.")
    print(f"File path: {csv_path.resolve()}")
    print(f"Records: {metadata['records']}")
    print(f"Metadata saved to: {metadata_path.resolve()}")

    # Display preview
    display(weather_df.head())

```

------------------------------------------------------------------------

## 7. Harmonize Scraped Dataset with Valid ISO2 Country Codes

To enable consistent integration across sources, we harmonize the scraped dataset with the validated ISO2 codes.

This ensures that every record in `scrapethissite_filtered.csv` aligns with the structure of the new weather dataset.

```{python}
# 7. Harmonize Scraped Dataset with Valid ISO2 Codes ---------------------------
"""
Filter and enrich the scraped dataset with validated ISO2 codes,
ensuring consistency across data sources before SQL integration.
Duplicates in the capital list are removed to preserve a one-to-one merge.
"""

from pathlib import Path

# Normalize capital names to ensure consistent merging
scraped_df["capital"] = scraped_df["capital"].astype(str).str.strip().str.casefold()
capitals_clean["capital"] = capitals_clean["capital"].astype(str).str.strip().str.casefold()

# Remove duplicate capitals in the right dataset to ensure one-to-one mapping
dupes = capitals_clean[capitals_clean["capital"].duplicated(keep=False)]
if not dupes.empty:
    print(f"Warning: {len(dupes)} duplicate capital entries detected and removed.")
    capitals_clean = capitals_clean.drop_duplicates(subset="capital", keep="first")

# Remove any pre-existing country_code column to prevent duplication
scraped_df = scraped_df.drop(columns=["country_code"], errors="ignore")

# Perform merge (now guaranteed to be many-to-one)
filtered_scraped = scraped_df.merge(
    capitals_clean,
    on="capital",
    how="inner",
    validate="m:1"
)

# Restore capitalization for readability
filtered_scraped["capital"] = filtered_scraped["capital"].str.title()

# Reorder columns
cols = list(filtered_scraped.columns)
if "country_name" in cols and "country_code" in cols:
    cols.insert(cols.index("country_name") + 1, cols.pop(cols.index("country_code")))
    filtered_scraped = filtered_scraped[cols]

# Reporting and validation
initial_count = len(scraped_df)
final_count = len(filtered_scraped)
loss_pct = (1 - final_count / initial_count) * 100

print(f"Initial records: {initial_count}")
print(f"Retained after harmonization: {final_count} ({100 - loss_pct:.2f}% retained)")
print(f"Harmonized dataset contains {final_count} valid ISO2-coded records.")

# Define export path
output_dir = project_root / "data" / "processed"
output_dir.mkdir(parents=True, exist_ok=True)
scrape_output_path = output_dir / "scrapethissite_filtered.csv"

# Export harmonized dataset
filtered_scraped.to_csv(scrape_output_path, index=False, encoding="utf-8")

# Verify export
if not scrape_output_path.exists() or scrape_output_path.stat().st_size == 0:
    raise IOError(f"Export failed or file is empty: {scrape_output_path}")

print(f"Harmonized dataset saved to: {scrape_output_path.resolve()}")

filtered_scraped.head()

```

## 8. Store Results in SQLite Database

Finally, both datasets — the harmonized scraped data and the live weather data — are stored in a unified SQLite database.

This step consolidates all information into a single, query-ready structure for further analysis in the upcoming SQL integration notebook.

```{python}
# 8. Store Results in SQLite Database -----------------------------------------
"""
Store both the harmonized scraped data and the essential weather metrics
in a unified SQLite database for analytical integration.
If the weather dataset is empty (e.g., API unavailable), store only the
scraped data to preserve database consistency.
"""

import sqlite3

# Define database path
db_path = project_root / "data" / "global_data.db"
db_path.parent.mkdir(parents=True, exist_ok=True)

# Handle potential absence of weather_df (if API failed)
if "weather_df" not in locals() or weather_df.empty:
    print("Warning: weather_df is empty or undefined. Only scraped dataset will be stored.")
    with sqlite3.connect(db_path) as conn:
        filtered_scraped.to_sql("scrapethissite", conn, if_exists="replace", index=False)
    print(f"Database created with single table: scrapethissite ({len(filtered_scraped)} rows)")
else:
    # Select only the relevant weather columns for SQL integration
    weather_sql = weather_df[["capital", "country_code", "temp_celsius", "humidity"]].copy()

    # Validate structure before writing
    if filtered_scraped.empty or weather_sql.empty:
        raise ValueError("Attempted to write empty tables to database. Check prior steps before database export.")

    expected_weather_cols = {"capital", "country_code", "temp_celsius", "humidity"}
    missing = expected_weather_cols - set(weather_sql.columns)
    if missing:
        raise KeyError(f"Missing expected columns in weather dataset: {missing}")

    # Store both datasets in SQLite
    with sqlite3.connect(db_path) as conn:
        filtered_scraped.to_sql("scrapethissite", conn, if_exists="replace", index=False)
        weather_sql.to_sql("weather", conn, if_exists="replace", index=False)

    # Confirm export and report structure
    print("\n--- Database Summary ---")
    print("Database successfully created and populated.")
    print(f"Path: {db_path.resolve()}")
    print(f"Tables stored: scrapethissite ({len(filtered_scraped)} rows), weather ({len(weather_sql)} rows)")

    # Display quick verification of the first few records
    with sqlite3.connect(db_path) as conn:
        sample_weather = pd.read_sql_query("SELECT * FROM weather LIMIT 5;", conn)
        print("\nSample from 'weather' table:")
        display(sample_weather)

```

Both tables (`scrapethissite` and `weather`) can now be accessed using standard SQL queries, ensuring seamless integration for the next notebook.

------------------------------------------------------------------------

## 9. Summary

This notebook constitutes **Stage 2 of the Data Integration Pipeline – From Web to Database**. It enriches the scraped dataset from Stage 1 with real-time weather information retrieved through the OpenWeather API and stores both sources in a unified SQLite database.

The consolidated datasets (`scrapethissite` and `weather`) are now ready for **Stage 3**, where SQL queries will combine them with World Bank economic indicators to perform analytical integration and generate reproducible insights.

------------------------------------------------------------------------